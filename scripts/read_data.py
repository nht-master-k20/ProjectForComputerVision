import pandas as pd
from sklearn.model_selection import train_test_split
import os
import matplotlib.pyplot as plt
import seaborn as sns
import random
import cv2
import albumentations
from tqdm import tqdm
import numpy as np
from concurrent.futures import ProcessPoolExecutor
import functools


class ReadData:
    GT_PATH = 'dataset/ISIC_2024_Training_GroundTruth.csv'
    IMAGES_DIR = 'dataset/ISIC_2024_Training_Input'

    # Th∆∞ m·ª•c ch·ª©a ·∫£nh ƒë√£ l√†m s·∫°ch
    CLEAN_IMAGES_DIR = 'dataset/ISIC_2024_Clean_Input'
    # Th∆∞ m·ª•c ch·ª©a ·∫£nh tƒÉng c∆∞·ªùng (ch·ªâ cho t·∫≠p Train)
    AUG_CLEAN_IMAGES_DIR = 'dataset/ISIC_2024_Augmented_Clean'

    CLASS_MAP = {0: 'L√†nh t√≠nh', 1: '√Åc t√≠nh'}

    CSV_OUTPUT_DIR = 'dataset_splits'
    ID_COLUMN = 'isic_id'
    TARGET_COLUMN = 'malignant'

    @classmethod
    def load_isic_metadata(cls) -> pd.DataFrame or None:
        try:
            df = pd.read_csv(cls.GT_PATH)
            df['image_path'] = df[cls.ID_COLUMN].apply(lambda x: os.path.join(cls.IMAGES_DIR, f"{x}.jpg"))
            print(f"‚úÖ T·∫£i th√†nh c√¥ng {len(df)} b·∫£n ghi.")
            return df
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i metadata: {e}")
            return None

    @classmethod
    def split_data(cls, df: pd.DataFrame, test_size=0.2, val_size=0.1, random_state=42):
        """Chia d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω ƒë·ªÉ tr√°nh r√≤ r·ªâ th√¥ng tin (Data Leakage)"""
        if (test_size + val_size) >= 1.0:
            raise ValueError("T·ªïng test_size v√† val_size ph·∫£i < 1.0")

        # Stratify split ƒë·ªÉ gi·ªØ nguy√™n t·ªâ l·ªá 99.9% vs 0.1%
        train_val_df, test_df = train_test_split(
            df, test_size=test_size, stratify=df[cls.TARGET_COLUMN], random_state=random_state
        )
        relative_val_size = val_size / (1.0 - test_size)
        train_df, val_df = train_test_split(
            train_val_df, test_size=relative_val_size, stratify=train_val_df[cls.TARGET_COLUMN],
            random_state=random_state
        )

        print(f"üìä Split Stats: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}")
        return train_df, val_df, test_df

    @staticmethod
    def remove_hair(image: np.ndarray) -> np.ndarray:
        """
        X√≥a l√¥ng v·ªõi kernel 5x5 v√† x·ª≠ l√Ω nh·∫π nh√†ng ƒë·ªÉ gi·ªØ chi ti·∫øt v·∫øt th∆∞∆°ng.
        """
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            # Kernel 5x5 theo y√™u c·∫ßu
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))

            # BlackHat transform ƒë·ªÉ t√¨m l√¥ng
            blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)

            # Gaussian blur nh·∫π ƒë·ªÉ gi·∫£m nhi·ªÖu
            blackhat = cv2.GaussianBlur(blackhat, (3, 3), 0)

            # Thresholding
            _, thresh = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)

            # Inpainting
            inpainted = cv2.inpaint(image, thresh, 1, cv2.INPAINT_TELEA)
            return inpainted
        except Exception:
            return image  # Fallback n·∫øu l·ªói

    # --- MULTIPROCESSING WORKER ---
    @staticmethod
    def _clean_single_image(row_tuple, output_dir):
        """H√†m x·ª≠ l√Ω 1 ·∫£nh (Static method ƒë·ªÉ picklable cho Multiprocessing)"""
        idx, row = row_tuple
        orig_path = row['image_path']
        filename = os.path.basename(orig_path)
        save_path = os.path.join(output_dir, filename)

        # N·∫øu ·∫£nh ƒë√£ t·ªìn t·∫°i th√¨ b·ªè qua (Resume capability)
        if os.path.exists(save_path):
            return save_path

        try:
            img = cv2.imread(orig_path)
            if img is not None:
                # Resize v·ªÅ 256 tr∆∞·ªõc khi remove hair ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω 400k ·∫£nh
                # img = cv2.resize(img, (256, 256))

                clean_img = ReadData.remove_hair(img)
                cv2.imwrite(save_path, clean_img)
                return save_path
        except Exception:
            pass
        return orig_path  # Tr·∫£ v·ªÅ ·∫£nh g·ªëc n·∫øu l·ªói

    @classmethod
    def clean_dataset_parallel(cls, df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
        """L√†m s·∫°ch d·ªØ li·ªáu s·ª≠ d·ª•ng ƒëa lu·ªìng (ProcessPoolExecutor)"""
        os.makedirs(output_dir, exist_ok=True)

        print(f"üöÄ ƒêang x·ª≠ l√Ω ƒëa lu·ªìng {len(df)} ·∫£nh v√†o: {output_dir}...")

        # S·ª≠ d·ª•ng s·ªë core CPU t·ªëi ƒëa - 1 ƒë·ªÉ tr√°nh treo m√°y
        max_workers = max(1, os.cpu_count() - 1)

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # C·ªë ƒë·ªãnh tham s·ªë output_dir
            worker = functools.partial(cls._clean_single_image, output_dir=output_dir)

            # Ch·∫°y song song v√† hi·ªán thanh ti·∫øn tr√¨nh
            results = list(tqdm(executor.map(worker, df.iterrows()), total=len(df), unit="img"))

        df_clean = df.copy()
        df_clean['image_path'] = results
        return df_clean

    @staticmethod
    def get_augmentation_pipeline(img_size=256):
        """Pipeline n√¢ng cao cho da li·ªÖu"""
        return albumentations.Compose([
            albumentations.Resize(img_size, img_size),

            # H√¨nh h·ªçc (Geometric)
            albumentations.HorizontalFlip(p=0.5),
            albumentations.VerticalFlip(p=0.5),
            albumentations.RandomRotate90(p=0.5),
            albumentations.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),

            # Bi·∫øn d·∫°ng (Distortion) - Gi√∫p model h·ªçc t√≠nh co gi√£n c·ªßa da
            albumentations.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),
            albumentations.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=0.3),

            # M√†u s·∫Øc (Color)
            albumentations.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3),
            albumentations.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),
            # p=0.2 theo y√™u c·∫ßu

            # Nhi·ªÖu (Noise) - Optional, th√™m v√†o n·∫øu mu·ªën robust h∆°n
            # albumentations.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
        ])

    @classmethod
    def balance_and_augment(cls, train_df: pd.DataFrame) -> pd.DataFrame:
        """
        Augment ch·ªâ √°p d·ª•ng cho Train Set.
        L∆∞u √Ω: V·ªõi 400k ·∫£nh, b∆∞·ªõc n√†y c√≥ th·ªÉ sinh ra R·∫§T NHI·ªÄU ·∫£nh.
        """
        os.makedirs(cls.AUG_CLEAN_IMAGES_DIR, exist_ok=True)

        class_counts = train_df[cls.TARGET_COLUMN].value_counts()
        majority_label = class_counts.idxmax()
        minority_label = class_counts.idxmin()

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng sinh th√™m ƒë·ªÉ tr√°nh tr√†n ·ªï c·ª©ng (v√≠ d·ª• max 50k ·∫£nh th√™m)
        # B·∫°n c√≥ th·ªÉ b·ªè limit n√†y n·∫øu ·ªï c·ª©ng ƒë·ªß l·ªõn
        n_diff = class_counts[majority_label] - class_counts[minority_label]
        n_to_generate = n_diff  # Ho·∫∑c min(n_diff, 50000)

        if n_to_generate <= 0:
            return train_df

        print(f"üé® Augmenting: Sinh th√™m {n_to_generate} ·∫£nh cho l·ªõp {minority_label}...")

        minority_df = train_df[train_df[cls.TARGET_COLUMN] == minority_label]
        minority_paths = minority_df['image_path'].tolist()
        pipeline = cls.get_augmentation_pipeline()

        new_records = []

        # D√πng tqdm ƒë·ªÉ theo d√µi ti·∫øn ƒë·ªô sinh ·∫£nh
        for i in tqdm(range(n_to_generate), unit="img"):
            src_path = random.choice(minority_paths)
            try:
                img = cv2.imread(src_path)
                if img is None: continue
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                # Apply Augmentation
                augmented = pipeline(image=img)['image']

                # Save
                fname = f"aug_{i}_{os.path.basename(src_path)}"
                save_path = os.path.join(cls.AUG_CLEAN_IMAGES_DIR, fname)
                cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))

                new_records.append({
                    cls.ID_COLUMN: f"aug_{i}",
                    cls.TARGET_COLUMN: minority_label,
                    'image_path': save_path
                })
            except:
                continue

        return pd.concat([train_df, pd.DataFrame(new_records)], ignore_index=True)

    @classmethod
    def run(cls, mode='raw', clean=True):
        # 1. Load Metadata
        full_df = cls.load_isic_metadata()
        if full_df is None: return False

        # 2. Split Data (QUAN TR·ªåNG: Split tr∆∞·ªõc khi l√†m b·∫•t c·ª© g√¨ ƒë·ªÉ tr√°nh Leakage)
        train_df, val_df, test_df = cls.split_data(full_df)

        # 3. Clean Data (√Åp d·ª•ng cho c·∫£ 3 t·∫≠p, nh∆∞ng ƒë·ªôc l·∫≠p)
        if clean:
            print("\nüßπ B·∫Øt ƒë·∫ßu quy tr√¨nh l√†m s·∫°ch (Multiprocessing)...")
            # ProcessPoolExecutor ƒë∆∞·ª£c g·ªçi b√™n trong h√†m n√†y
            train_df = cls.clean_dataset_parallel(train_df, cls.CLEAN_IMAGES_DIR)
            val_df = cls.clean_dataset_parallel(val_df, cls.CLEAN_IMAGES_DIR)
            test_df = cls.clean_dataset_parallel(test_df, cls.CLEAN_IMAGES_DIR)

        # 4. Augment Data (CH·ªà √ÅP D·ª§NG CHO TRAIN SET)
        if mode == 'augment':
            print("\nüé® B·∫Øt ƒë·∫ßu quy tr√¨nh Augmentation (Ch·ªâ Train Set)...")
            train_df = cls.balance_and_augment(train_df)

        # 5. Save CSVs
        output_dir = cls.CSV_OUTPUT_DIR
        os.makedirs(output_dir, exist_ok=True)

        prefix = "clean_" if clean else "raw_"
        suffix = "_augmented" if mode == 'augment' else ""

        print(f"\nüíæ L∆∞u file CSV t·∫°i {output_dir}...")
        train_df.to_csv(os.path.join(output_dir, f'{prefix}train{suffix}.csv'), index=False)
        val_df.to_csv(os.path.join(output_dir, f'{prefix}val.csv'), index=False)
        test_df.to_csv(os.path.join(output_dir, f'{prefix}test.csv'), index=False)

        print("‚úÖ Ho√†n t·∫•t to√†n b·ªô quy tr√¨nh ReadData.")
        return True